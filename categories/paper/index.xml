<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>paper on Pan'Log</title><link>https://payne4handsome.github.io/categories/paper/</link><description>Recent content in paper on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 12 Nov 2023 16:51:28 +0800</lastBuildDate><atom:link href="https://payne4handsome.github.io/categories/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS</title><link>https://payne4handsome.github.io/posts/papers/2023-11-12-pink/</link><pubDate>Sun, 12 Nov 2023 16:51:28 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-11-12-pink/</guid><description>Title: PINK: UNVEILING THE POWER OF REFERENTIAL COMPREHENSION FOR MULTI-MODAL LLMS 作者: Shiyu Xuan 发表日期: 2023-10-01 一、Introduction 背景知识
Referring：识别图片中具体的目标类别（包括给定point、bounding box、mask等） Grounding：给定文本描述，输出bounding box 简单来讲，Referring是给定坐标，输出文本（类别或者描述）；Grounding是给定文本，输出坐标
1.1 该论文试图解决什么问题？ 大部分的MLLM缺乏指代能力（Referential Comprehension (RC)），这篇提出一个新方法增强MLLM的RC能力。这篇文章中RC即包括Referring能力也包括Grounding能力
1.2 Key Contributions 提出pink增加MLLM的RC能力 用设计的各种RC任务，以一个低成本的方式构建质量微调数据集。为了进一步提升模型RC能力，提出自一致提升方法（self-consistent bootstrapping ）扩展一个数据集的dense object annotations到高质量的referring-expression-bounding-box pair。 端到端训练框架，两个模态从指令微调中都收益（视觉、LLM加入了可学习参数，Adapter） SOTA（在某些方面比Kosmos-2还强） 介绍中的要点 传统VQA和RC的区别 传统的VQA是image-level的, RC VQA是更细粒度的 Method 整体架构 右边的self-consistent bootstrapping包括两步（1）grounding caption： 给定框生成caption，（2）visual grounding： 给定caption预测框
左边的模型结构包括visual encoder，projection layer，decoder-only LLM。
Training Pipeline：（1）第一阶段：只训练projection layer；（2）第二阶段：冻结e visual encoder和LLM。 训练新添加的Adapters参数（viusal encoder和LLM都会新加一些参数）和projection layer
指令微调数据集构建 设计的RC task包括如下（前3个是已经存在工作的方法，后面的是作者后设计的）
visual relation reasoning visual spatial reasoning PointQA Visual Relation Reasoning Coarse Visual Spatial Reasoning：define four coarse spatial positions as top-left, top-right, bottom-left, and bottom-right.</description></item><item><title>Flamingo</title><link>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</link><pubDate>Sun, 24 Sep 2023 17:40:40 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-09-24-flamingo/</guid><description>Title: Flamingo: a Visual Language Model for Few-Shot Learning 作者: Jean-Baptiste Alayrac, Jeff Donahue 发表日期: 2022.11 一、Introduction 1.1 该论文试图解决什么问题？ 多模领域的few-shot问题
1.2 Key Contributions 提出Flamingo模型，通过几个示例就可执行各种多模任务。由于架构的创新，Flamingo可以处理随意的图片（可以多张图片）和文本 通过few-shot学习，定量评估Flamingo是如何迁移到其他各种任务的 通过few-shot学习，Flamingo在16任务中的6个任务(6个人任务是finetune过的)取到SOTA。Flamingo可以在其他数据集上通过fine-tune取到SOTA。 Method Flamingo架构总览如下图 从图中可以看到Flamingo架构有两个关键点组件，Perceiver Resampler和Gated XATTN-DENSE
Perceiver Resampler: 任意数量的图片或者视频经过视觉模型编码后，再通过Pereiver Resampler输出固定数量的visual tokens。注：该模块决定了Flamingo可以处理多张图片的能力（即具有few-shot的能力） Gated XATTN-DENSE: 主要是指cross attention的基础加入门机制(tanh(a), a初始化为0)，可以提升性能和训练的稳定性 Visual processing and the Perceiver Resampler Perceiver Resampler示意图如下，学习DETR的query机制，有几个query，输出就是几个visual token（论文中为5） Conditioning frozen language models on visual representations 在Transformer中的cross attention的基础加入门机制 Multi-visual input support: per-image/video attention masking 网络上爬取的文档是图片和文本交错的信息。该模块是用来控制当前文本token可以注意到的图片（离当前文本token最近的上一个图片）
Training on a mixture of vision and language datasets Flamingo训练采用了三个数据集：</description></item><item><title>MME</title><link>https://payne4handsome.github.io/posts/papers/2023-09-08-mme/</link><pubDate>Fri, 08 Sep 2023 11:29:11 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-09-08-mme/</guid><description>Title: MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models 作者: Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin1Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji; Tencent Youtu Lab , Xiamen University 发表日期: 2023.7 项目主页：MME Note: 项目主页加入了新的多模模型，得分已经远远超过论文的那个几个模型 一、Introduction 缩写
LLM: Large Language Model MLLM: Multimodal Large Language Model LLM 三个代表性的能力: In-Context Learning(ICL), instruction following, Chain-of-Thought (CoT)
1.1 该论文试图解决什么问题？ 多模模型缺乏一个全面的评估benchmark，该论文首次提出多模大模型的评估benchmark MME。在14个子任务上度量多模大模型的感知和认知能力。</description></item><item><title>IMAGEBIND: One Embedding Space To Bind Them All</title><link>https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/</link><pubDate>Mon, 26 Jun 2023 23:15:33 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-26-imagebind/</guid><description>一、Introduction 1.1 该论文试图解决什么问题？ 该论文主要解决的多模态对齐的问题，该论文将图片（视频）、文本、音频、深度图、热力图（thermal）、IMU六种模态的特征对齐在一个空间。 所以IMAGEBIND可以做跨模态召回（cross-modal retrieval）、简单相加融合模态信息（composing modalities with arithmetic）、跨模态检测和生成（cross-modal detection and generation）等任务。另外IMAGEBIND的few-shot能力也不错
补充说明 目前主流的方法还是将图片和文本（或者声音）对齐，比如CLIP（Audio-CLIP）。但是没有像IMAGEBIND方法这样讲6种模态的特征对齐，本质原因是没有6种模态对齐的训练数据（指一条样本对包含的6种模态数据完成对应）。但是每一种模态和图片成对的数量是够的，就是（图片-文本）、（图片-音频）、（图片-深度图）、（图片-热力图）、（图片-IMU）这种成对的数据是够的。IMAGEBIND就是把所有模态的数据都和图片这个模态的数据进行对齐。那么比如（文本-音频）、（文本-深度图）等跨模态的数据就也对齐的。这种在数学上叫做传递性，因为所有模态的相似度量是用的cosine距离，这个度量方式就是可传递的，所以IMAGEBIND能把这么多模态对齐是显然的。 emergent zero-shot：由于IMAGEBIND是将其他模态和图片模态配对然后训练，其它的模态对是没有进行训练的，比如（文本-音频）、（文本-深度图）。所以（文本-音频）的召回或者分类能力，IMAGEBIND叫做涌现的zero-shot能力。 至于网络结构损失函数等，并没有新的东西。甚至图像-文本的模态对齐就是用的CLIP（文中用的OPEN-CLIP），直接frozen掉没有训练 Method ImageBind的网络结构没有什么新的架构，无非就是不同规模的VIT结构。损失与CLIP的对比损失不同，用的是InfoNCE loss。公式如下：
其中$q_i$, $k_i$分别表示图片、其它模态数据经过encoder后的embedding。$\tau$表示温度，用于控制softmax后的平滑程度。
Experiments ImageBind的应用 跨模态召回 embeding相加就等价于语义的相加 声音生产图片 ImageBind使用的数据样例 都是自然与图片配对的数据 ImageBind使用的评测数据集 可以看到都是分类、召回类的任务 Emergent zero-shot分类能力 音频的分类任务重ImageBind与AudioCLIP对比，但是AudioCLIP是直接在（text, audio）成对的数据上训练的，且AudioCLIP用到了AS类别信息，所以ImageBind提到AudioCLIP的指标不能算zero-shot，所以AudioCLIP的指标对ImageBind的高一点 文本召回视频 A: Audio, V:Video。 可以看到用音频和图片的联合embedding取得了最好的效果。 Few-shot能力 使用不同规模的Image Encoder 关于温度（损失函数中用于控制平衡的参数，见损失公式）$\tau$的影响</description></item><item><title>Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title><link>https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/</link><pubDate>Mon, 19 Jun 2023 14:01:46 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-19-i-jepa/</guid><description>Title: 从图像的联合-embedding预测架构中自监督学习 作者: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski1Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas 发表日期：2023.4 一、Introduction 1.1 该论文试图解决什么问题？ 不依赖于手工的数据增强，I-JEPA可以学习到更高阶的语义图像特征。同时I-JEPA还具有可伸缩性、计算高效等优点。
1.2 以往方法存在的问题 Invariance-based methods
基本思想：同一张图片的不同视角（不同数据增强方式）的embedding是相似的。 缺点：引入很强的偏置（biases），对下游任务有害、甚至对不同分布的预训练任务也有害。 优点：学习高层的语义信息 generative methods
基本思想：删除图像的一部分，然后预测缺失的部分。 缺点：效果差于Invariance-based的方法，且获得底层的语义信息。 Key Contributions I-JEPA 学习强大的开箱即用（off-the-shelf）的特征表示，不需要手工的view augmentations。并且由于MAE，半监督等方法 在low-level视觉任务，像目标统计、深度估计，I-JEPA也取得了更好性能 I_JEPA是可伸缩（模型越大，效果越好）且高效（计算高效）的，体现在需要更少的GPU hours，比iBOT快2.5倍，10倍的高效与MAE。 背景知识 常规的自监督范式可以归为以下三类。自监督基本思想都是一样的，incompatible inputs（负样本对）的损失大（high energy）， compatible inputs 损失小（low energy） Joint-Embedding Architectures: 正样本对encoder后，特征是相似的（打高分），负样本对，特征不相似（打低分） Generative Architecture: 直接从一个隐变量中重构，类似于VAE Joint-Embedding Predictive Architectures: 与Joint-Embedding Architectures类似，只不过对比损失的是两个embedding Method 核心思想如下图所示： 阐述：从一张图片随机采样M（论文中M=4）个区域， 这些区域的长宽比在（0.75, 1.5）之间，然后随机缩放，缩放比在（0.15, 0.2）之间。然后这M个区域经过target encoder，得到特征表示。这些特征表示就是需要预测的东西（与直接预测像素不同）。context经过context encoder,然后加上位置编码去预测target网络得到的特征。该图画的有点问题，context encoder和target encoder的输入图片应该是没有交集的，这个论文其它部分有说。采用的损失是$L_2$损失</description></item><item><title>HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation</title><link>https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/</link><pubDate>Mon, 05 Jun 2023 11:06:46 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-06-05-hilo/</guid><description>一、Introduction 任务定义 SGG: 给定一张图片，抽取三元组：主体（subjects）、客体（objects）、关系（relations）。其中主体、客体用bounding box框出来 PSG: SGG是用bounding box将主体、客体标出来，PSG用全景分割（panoptic segmentation）来替代bounding box
1.1 该论文试图解决什么问题？ 以往的scene graph generation任中，关系存在长尾问题，本文提出HiLo架构可以有效解决该问题。
1.2 以往方法存在的问题 关系的类别有一个长尾效应问题，以往的方法更倾向于预测高频的关系（成为biasd methods） 主体-客体对的关系存在语义重叠（有多种语义关系）,以往的方法倾向于只预测一种 二、Method 2.1 biased &amp;amp; unbiased method biased方法：指经过统计，有些关系出现的次数是远远高于其他关系的，那么模型在预测的时候会倾向于高频关系的预测，具有这种特性的方法称为biased method。 以下是biased method、unbiased method和本文的方法预测的差异 biased method： 预测的结果是向looking at、 beside这种常见的高频的关系 unbiased method: 预测的结果主要的是向chasing、playing这类低频的词 HiLo：既有低频的关系也有高频关系 2.2 overview 整体结构如下（还是比较复杂的） 先看中间的结构，该结构来自于mask2former，mask2former的思想又来自于maskfomer和DETR，所以想要清楚的了解该结构，需要把这3篇论文看一下。下面只是简述。 图（b）解释
该网络结构分为上下两个分支，其中上面（H-L）部分用于预测低频关系,下面（L-H）部分用预测高频关系。 Triplet Query: 源自DETR，相当于可学习的位置编码；固定数量（mask2former中取100）；经过decoder后和Pixel Decoder得到的feature相乘，得到N个mask Task Heads: 这里需要产生3个类别（subject、object、related）的预测，网络结构：three linear classifiers ；2个mask（subject和object的mask）的预测， 网络结构：2个MLP后得到的embeding与feature相乘得到mask Masked relation attention： 该结果没有出现在图中，但是这个mask attention是mask2former相较于maskformer最大的创新点，核心思想就是在计算注意力事，每个object只和做注意力计算，而不是和全图做注意力 该处loss如下：
$$L_{baseline}=\lambda_1 \cdot L_{so_{cls}}+ \lambda_2 \cdot L_{so_mask}+ \lambda_2 \cdot L_{re\_{cls}}$$</description></item><item><title>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title><link>https://payne4handsome.github.io/posts/papers/2023-05-22-blip/</link><pubDate>Mon, 22 May 2023 14:37:57 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-22-blip/</guid><description>Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）其中一方面表现优秀。 本文主要解决问题有二。
提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块：从合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数
主要分为三个模块
Unimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后各自(指参数不共享)的finetune captioner模块和filter模块。</description></item><item><title>BLIP-2:Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title><link>https://payne4handsome.github.io/posts/papers/2023-05-15-blip2/</link><pubDate>Mon, 15 May 2023 16:00:20 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-15-blip2/</guid><description> Title: BLIP-2: 用冻结的图像编码模型和大语言模型引导文本-图像预训练 作者: Junnan Li Dongxu Li Silvio Savarese Steven Hoi；Salesforce Research 发表日期：2023.5 github: https://github.com/salesforce/LAVIS/tree/main/projects/blip2 该论文试图解决什么问题？ 由于端到端的训练, 预训练视觉-语言模型代价变的非常高昂。这篇论文提出了BLIP-2, 一个通用的、有效的预训练策略: 其从现成的冻结的视觉模型和冻结的大语言模型，引导视觉-语言（vision-language）模型的预训练。该方法解决的跨模态对齐(视觉模型和LLM)问题。
应用：Instructed Zero-shot Image-to-Text Generation 先展示一下BLIP2的强大能力，这是BLIP2最亮眼的地方。
信息检索能力，利用LLM强大的知识库 事实推理能力 开放生成能力 Method 整体架构
两阶段策略，预训练一个轻量级Q-Former模块去连接两种模态的gap。
第一阶段：从一个frozen image encoder中引导vision-language表示学习（representation learning）。
第二阶段：从一个frozen LLM中引导vision-to-language的生成学习（generative learning）
第一个阶段：图片-文本表示学习（vision-language representation learning） note: Q-Former的输出维度Z(32768)远远小于VIT-L/14(2571024)的维度 注意三个目标self-attention mask的不同
Q-Former作用：从图片中提取与文本最相关的特征
第二个阶段：图片到文本生成学习（vision-to-language generative pre-training） Q-Former后接入一个全连接层，用于使用LLM的输入。LLM model分为两类，一个像OPT只有Decoder模块，一个像FlanT5既有Encoder又有Decoder模块。
Experiments 在各个视觉-语言任务上的zero-shot能力 zero-shot VQA 参考文献 BLIP2：下一代多模态模型的雏形 多模态学习持续梳理</description></item><item><title>LoRA: Low-RanK Adaption Of Large Language Models</title><link>https://payne4handsome.github.io/posts/papers/2023-05-09-lora/</link><pubDate>Tue, 09 May 2023 21:28:47 +0800</pubDate><guid>https://payne4handsome.github.io/posts/papers/2023-05-09-lora/</guid><description>Title: LoRA: 大语言模型的低秩适配 作者: {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu 发表日期：2021.10 该论文试图解决什么问题？ 提出一个大模型的低秩适配方法去解决全量微调大模型时候需要全量更新模型参数、显存占用很大的问题。
Key Contributions 对于不同的下游任务，大模型的参数是共享的，变化的只不过是LoRA方法新引入的参数（即B、A参数矩阵）。所以如果有比较多的下游任务，大模型参数只需要保存一份，切换任务的时候也只需要切换一下B、A矩阵即可。大大减少了模型存储的空间和任务切换时候的负载 LoRA方法可以使训练更有效（耗时减少）、减少3倍的显存使用。因为不用保存原始大模型参数的梯度。eg，GPT-3训练需要1.2T显存，使用LoRA方法显存只需要350G左右 不增加推理耗时（上面已经提到） 可以和其他的适配方法结合，比如prefix-tuning Abstract &amp;amp; Introduction &amp;amp; Method NLP模型使用的一个通用范式是先选择一个大的在通用数据集上训练的预训练模型，然后再在一个特定任务上做fine-tune。 但是如果做全量的fine-tune，就要更新模型所有的参数。比如GPT-3有1750亿的参数。fine-tune需要更新1750亿的参数，这个操作是昂贵的。本文提出一个名为LoRA(Low-Rank Adaption)的方法：freeze 预训练模型的参数，在原有的模型结构中插入低秩分解矩阵（rank decomposition matrices）. 该方法可以极大的减少模型的训练参数。
方法示意图如下 右边橙色的为新引入的可训练的低秩矩阵，其它的为原始模型的参数。数学表达可能更清楚一点。原始模型的前向过程表达为
$$h = W_0x$$, 修改后的前向过程如下：
$$h = W_0x+\Delta Wx=W_0x+BAx$$
LoRA核心的方法就是改公式。在模型保存的时候可以将$W_0+\Delta W$保存（即加起来），所以改方法不会增加模型的推理耗时
Experiments 与不同适配方法在GLUE上的对比 在GPT-3上的适配效果对比 不同方法加大可训练参数量效果对比 Transformer结构为例，LoRA加到哪里更有效？ 参数总量不变（秩r改变），加的地方不一样。实验表明加到$W_q$,$W_v$上效果更好
r是不是越大越好？ 实验表明，r并不是越大效果越好，对于一些任务，r=4就足够了（取1效果也不错）。对于这个结论论文有一些说明，大致的意思就是r=4的时候，参数量已经够要学习的信息了，再打也是无非是引入冗余的信息罢了。这里解析的可以有失偏颇，感兴趣的参见原文为好。
CONCLUSION AND FUTURE WORK 关于未来的工作方向。
LoRA可以和其他迁移方法结合 fine-tuning或者LoRA背后的机制是不清楚的，如何将在预训练的时候学习到的特征迁移到下游任务？作者认为LoRA比full fine-tuning做更好。 作者将LoRA添加到参数矩阵，是通过穷尽、实验的方式，有没有更好的指导原则？ 既然LoRA可以通过添加一个低秩的矩阵就可以取到好的效果，那么原始的参数矩阵是不是也可以降低一下秩？。 第4点确实是一个比较好、且重要的研究方向。</description></item></channel></rss>