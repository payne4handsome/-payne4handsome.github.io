<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>机器学习 on Pan'Log</title><link>https://payne4handsome.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 机器学习 on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 27 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://payne4handsome.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>YOLOv4学习摘要</title><link>https://payne4handsome.github.io/posts/machine-learning/yolov4%E5%AD%A6%E4%B9%A0%E6%91%98%E8%A6%81/</link><pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/yolov4%E5%AD%A6%E4%B9%A0%E6%91%98%E8%A6%81/</guid><description>本文主要是学习yolov4论文的一些学习笔记，可能不会那么详细。所以在文章最后列出了一些参考文献，供大家参考 yolo系列发展历程
yolo系列 发表时间 作者 mAP FPS mark yolov1 2016 Joseph Redmon 63.4
(VOC 2007+2012) 45 (448*448) yolov2 2016 Joseph Redmon 21.6
(coco test-dev2015) 59(480*480) yolov3 2018 Joseph Redmon 33
(coco) 20(608608), 35(416416) yolov4 2020 Alexey Bochkovskiy、Chien-Yao Wang 43.5
(coco) 65(608*608) yolov5 2020 Glenn Jocher （Ultralytics CEO） 48.2 (coco) 没有实测数据 没有论文，学术界不认可；该模型有不同大小的模型 scaled-yolov4 2021 Chien-Yao Wang、 Alexey Bochkovskiy 47.8
(coco) 62（608*608） 该模型有不同大小的模型 插曲，与中心思想无关 我们看到yolov1到v4 是由不同作者完成的，因为yolo 原作者已经在2018年推出CV界了，在yolov3论文的最后作者是这么写的 他发现他的研究成果被应用于战争，还有一些隐私的问题，所以他决定推出了&amp;hellip;&amp;hellip; yolo系统v1-v3由原作者Joseph Redmon完成，v4由AB(Alexey Bochkovskiy)完成，AB是参与前几个系列yolo代码的开发的，并且yolov4是得到原作者的认可的，在作者的原网页上有引用。
代码实现</description></item><item><title>Transformer in pytorch</title><link>https://payne4handsome.github.io/posts/machine-learning/transformer-in-pytorch/</link><pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/transformer-in-pytorch/</guid><description>一 Transformer overview 本文结合pytorch源码以尽可能简洁的方式把Transformer的工作流程讲解以及原理讲解清楚。全文分为三个部分
Transformer架构：这个模块的详细说明 pytorch中Transformer的api解读 实际运用：虽然Transformer的api使用大大简化了打码量，但是还有需要自已实现一些代码的 Transformer架构 Transformer结构如下： ![image.png](/Transformer in pytorch/8596800-4764969fee815f44.png) Transformer的经典应用场景就是机器翻译。 整体分为Encoder、Decoder两大部分，具体实现细分为六块。
输入编码、位置编码
Encoder、Decoder都需要将输入字符进行编码送入网络训练。
Input Embeding:将一个字符（或者汉字）进行编码，比如“我爱中国”四个汉字编码后会变成（4，d_model）的矩阵，Transformer中d_model等于512，那么输入就变成（4，512）的矩阵，为了方便叙述，后面都用（4，512）来当成模型的输入。
positional encoding:在Q、K、V的计算过程中，输入单词的位置信息会丢失掉。所以需要额外用一个位置编码来表示输入单词的顺序。编码公式如下
$PE_{pos,2i}=sin(pos/1000^{2i/d_{model}})$
$PE_{pos,2i+1}=cos(pos/1000^{2i/d_{model}})$
其中，pos:表示第几个单词，2i,2i+1表示Input Embeding编码维度（512）的偶数位、奇数位。 论文中作者也试过将positional encoding变成可以学习的，但是发现效果差不多；而且使用硬位置编码就不用考虑在推断环节中句子的实际长度超过训练环节中使用的位置编码长度的问题；为什么使用sin、cos呢？可以有效的考虑句中单词的相对位置信息
多头注意力机制（Multi-Head Attention）
多头注意力机制是Transformer的核心，属于Self-Attention（自注意力）。注意只要是可以注意到自身特征的注意力机制就叫Self-Attention，并不是只有Transformer有。 示意图如下
![image.png](/Transformer in pytorch/8596800-993737ea7c1e190a.png)
Multi-Head Attention的输入的Q、K、V就是输入的（4,512）维矩阵，Q=K=V。然后用全连接层对Q、K、V做一个变换，多头就是指的这里将输入映射到多个空间。公式描述如下：
$MultiHead(Q,K,V)=Concat(head_1, head_2,&amp;hellip;, head_n)W^o$
其中 $head_i=Attention(QW^Q_i, KW^K_i, VW^V_i)$
其中 $Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$
其中$W^Q_i\in R^{d_{model}*d_k}, W^K_i\in R^{d_{model}*d_k}, W^V_i\in R^{d_{model}d_v}, W^o\in R^{hd_vd_{model}}$, 论文中h=8, $d_k=d_v=d_{model}/h=512/8=64$
$QK^T$称为注意力矩阵（attention）,表示两个句子中的任意两个单词的相关性。所以attention mask不一定是方阵。
前向传播模块
Q、K、V经过Multi-Head Attention模块再加上一个残差跳链，维度不变，输入维度是（4，512），输出维度还是（4,512），只不过输出的矩阵的每一行已经融合了其他行的信息（根据attention mask）。 这里前向传播模块是一个两层的全连接。公式如下：
$FFN(x)=max(0, xW_1+b_1)W_2+b_2$, 其中输入输出维度为$d_model=512$, 中间维度$d_{ff}=2048$
带Mask的多头注意力机制
这里的Mask Multi-head Attention与步骤2中的稍有不同。“我爱中国”的英文翻译为“I love china”。 在翻译到“love”的时候，其实我们是不知道“china”的这个单词的，所以在训练的时候，就需要来模拟这个过程。即用一个mask来遮住后面信息。这个mask在实际实现中是一个三角矩阵（主对角线及下方为0，上方为-inf）， 定义为$attention_mask$大概就长下面这个样子 !</description></item><item><title>机器学习基础之反向传播</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</link><pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</guid><description>机器学习基础二-反向传播 神经网络之所以可以训练，得益于与Hinton在1986年提出的反向传播算法。反向传播背后的数学原理就是链式法则。本文会具体的实例来演示反向传播具体的计算过程，让大家有一个实际的印象。文中公式都是一个字符一个字符敲出来的，转载请注明出处。文中参考了其他作者的一些例子和数据，均在参考文献中列出。 一个简单的两层神经网络如上图所示。其中
$i_1, i_2$ : 训练时候实际的输入
$o_1, o_2$ : 训练时候实际的输出（ground truth）
$out_{o1}, out_{o2}$: 模型预测的结果
我们的目的就是通过反向传播，更新模型的参数$w_i,b_i$, 使得$out_{o1}, out_{o2}$尽可能的逼近$o_1, o_2$。
本例中激活函数用$sigmod(x)=\frac{1}{1+e^{-x}}$;
损失函数用$L_2=\frac{1}{2}*(target-output)^2$
前向传播 隐藏层计算 $net_{h1}=w_1i_1+w_2i_2+b_1\=0.150.05+0.20.1+0.35=0.3775$
$out_{h1}=sigmod(net_{h1})=\frac{1}{1+e^{-0.3775}}=0.593269992$
同理 $out_{h2}=0.596884378$
输出层计算 $net_{o1}=w_5out_{h1}+w_6out_{h2}+b_2\=0.40.593269992+0.450.596884378=1.105905967$
$out_{o1}=sigmod(net_{o1})=0.75136507$
同理 $out_{o2}=0.772928465$
最终我们看到前向传播的结果为（0.75136507，0.772928465）与我们的目标（0.01,0.99）差的比较多，所以接下来用反向传播算法学习
反向传播 要想反向传播，还需要两步（1）计算损失，（2）计算梯度。过程如下图所示 总的损失$E_{total}=E_{o1}+E_{o2}$,其中
$E_{o1}=\frac{1}{2}(target_{o1}-out_{o1})^2=\frac{1}{2}(0.01-0.75136507)^2=0.274811083\ E_{o2}=\frac{1}{2}(target_{o2}-out_{o2})^2=0.023560026\ E_{total}=E_{o1}+E_{o2}=0.274811083+0.023560026=0.298371109 $
计算参数$w_5$的梯度 根据链式法则，公式如下： $\frac{\partial E_{total}}{\partial w_5}=\frac{\partial E_{total}}{\partial out_{o1}}\frac{\partial out_{o1}}{\partial net_{o1}}\frac{\partial net_{o1}}{\partial w_5}$
$\frac{\partial E_{total}}{\partial out_{o1}}=(target_{o1}-out_{o1})*-1=-(0.01-0.75136507)=0.74136507$
$\frac{\partial out_{o1}}{\partial net_{o1}}=out_{o1}*(1-out_{o1})=0.75136507(1-0.75136507)=0.186815602$
$\frac{\partial net_{o1}}{\partial out_{w_5}}=out_{h1}=0.593269992$
所以$\frac{\partial E_{total}}{\partial w_5}=0.741365070.1868156020.593269992=0.082167041$
在计算后面参数的梯度时，都会需要用到$\frac{\partial E_{total}}{\partial net_{o1}}, \frac{\partial E_{total}}{\partial net_{o2}}$的值。我们把这个称为输出层的误差, 符号记为：</description></item><item><title>深度学习中的各种tricks</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8Dtricks/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8Dtricks/</guid><description>深度学习tricks汇总 网络结构 FPN（Feature Pyramid Networks ）、SPP（spatial pyramid pooling）、ASPP（atrous spatial pyramid pooling）、FPN（Feature Pyramid Networks ）、PAN（Pyramid Attention Network）、PSP（Pyramid Scene Parsing）或者Pyramid pooling module
SPP 主要用于图像分类和目标检测中，其实和PSP网络结构设计思想一样（SPP提出在前，PSP提出在后），PSP用于语义分割
PSP、PPM 使用不同尺度的pooling操作（论文中最终输出的特征图尺寸为1*1，2*2，3*3，6*6），然后使用1*1的卷积将channel数降为原来的1/N (这里的N为4，目前是不过多的增加计算量)，然后分别将输出上采样到原特征图大小，再concat拼接到一起
FPN FPN如上图（d）所示
PAN
整体结构： 其中 FPA结构： 其中： GAU结构： 参考文档：
https://medium.com/mlearning-ai/review-pan-pyramid-attention-network-for-semantic-segmentation-semantic-segmentation-8d94101ba24a
参考代码：
https://github.com/JaveyWang/Pyramid-Attention-Networks-pytorch
深度可分离卷积 损失函数 focal loss
$FL(p)=-(1-p)^\gamma log(p)$ 其中p为经过softmax后的输出。
当预测结果p越接近正确分类（接近1）的时候，会在原损失的基础上乘上$(1-p)^\gamma$的权重，该权重是一个比较小的值；相反如果预测结果p偏离正确分类的比较离谱，比如p结果0，那么$(1-p)^\gamma$就会比较大（相比于容易分的样本权重）。所以focal loss 在处理样本不均衡时候是一个比较不错的选择
smooth l1 loss $$loss = \left{ \begin{matrix} 0.5x^2, 当 x&amp;lt;1\ |x|-0.5, otherwise \end{matrix} \right. $$
改进了l2loss对异常比较敏感的问题（因为平方放大异常点的损失） 改进l1loss在零点不可能的问题 训练策略 学习率warmup Warmup策略顾名思义就是让学习率先预热一下，在训练初期我们不直接使用最大的学习率，而是用一个逐渐增大的学习率去训练网络，当学习率增大到最高点时，再使用学习率下降策略中提到的学习率下降方式衰减学习率的值
学习率 在整个训练过程中，我们不能使用同样的学习率来更新权重，否则无法到达最优点，所以需要在训练过程中调整学习率的大小。
参考百度总结的一些训练技巧</description></item><item><title>梯度下降优化方法概述</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/</guid><description>梯度下降是优化神经网络和机器机器学习算法的首选优化方法。本文重度参考SEBASTIAN RUDER的文章。对于英文比较好的同学请直接阅读原文。本文只为个人的学习总结，难免有所欠缺和不足。
一、梯度下降变种 根据训练数据集的大小，梯度下降有三种变体，但是本质是一样的，不一样的是每次使用多少条样本。如果内存一次可以计算所有样本的梯度，称为：批梯度下降（Batch gradient descent）；如果内存一次只允许一个样本，称为：随机梯度下降（Stochastic gradient descent）；大部分时候，内存一次是可以计算部分样本的，称为：最小批梯度下降（Mini-batch gradient descent）。三种变体的数据表达如下：
1.1批梯度下降(Vanilla gradient descent,又称Batch gradient descent) $\theta = \theta - \eta \cdot \nabla_\theta J( \theta)$
1.2随机梯度下降（Stochastic gradient descent） $\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})$
1.3最小批梯度下降（Mini-batch gradient descent） $\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})$
注意，在其他地方并没对上述三种变体做严格区别，统称为SGD（随机梯度下降），下文其余部分，我们也不加区分，统称为SGD
二、梯度下降的几种优化方法 传统的梯度下降法不能保证一个很好的收敛，而且有一些挑战需要被解决。
选择这个合适的学习率是比较困难的。特别是对一个新的模型和新数据集时候，我们是不知道选择什么样的学习率是合适的。只能不断的去尝试。 学习率调度算法可以在训练的过程中去调整模型的学习率。模型一开始的时候可以使用大一点的学习率，后面再使用小一点的学习率去微调模型。更好的方法是一开始也用一个小的学习率去warm-up训练，让参数先适应数据集。但是无论哪种学习率调度算法都需要预先定义调度算法，这种方法也是没有办法很好的适应模型的特征的、 对每一个参数都使用同样的学习率是不合适的。对于稀疏的数据或者特征非常不均衡的数据。最好是使用不同学习率学习不同频率的特征。 另外的挑战是对于高阶非凸的损失函数，往往会陷于局部极值点。还有一种鞍点的情况，模型也是很难学习的。此时损失函数在各个方向的梯度接近于0。SGD是很难逃脱与鞍点或者局部极值点的。 针对上面的一些问题，慢慢出现了一些针对梯度下降的优化方法。 在介绍SGD变种之前。先给出各个变种的一般范式。后天的各个变种优化方法都离不开这个范式。
(1)计算目标函数关于参数的梯度
$g_t = \nabla_\theta J( \theta)$
(2)根据历史梯度计算一阶和二阶动量(二阶指的是梯度的平方) $ m_t = \phi(g_1, g_2, &amp;hellip;, g_t) \ v_t = \psi(g_1, g_2, &amp;hellip;, g_t) $</description></item><item><title>pytorch ddp</title><link>https://payne4handsome.github.io/posts/machine-learning/pytorch-ddp/</link><pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/pytorch-ddp/</guid><description>(1) 关键概念
world_size: 集群中所有GPU的数量 rank: 范围[0, world_size-1], 表示GPU的编号 local_rank: GPU在每台机器上的编号 比如，两台机器，每台机器4块卡，那么world_size= 2*4, rank 取值范围 [0,1,2,3,4,5,6,7]， local_rank 取值范围[0,1,2,3] (2) torch.distributed.launch 启动集群参数
&amp;ndash;nnodes: 一共多少台机器 &amp;ndash;node_rank: 当前机器编号 &amp;ndash;nproc_per_node: 每台机器多少个进程 &amp;ndash;master_adderss: master节点ip地址 &amp;ndash;master_port: master节点端口 master节点的node_rank必须为0 command example:
python -m torch.distributed.launch --nnodes=2 --node_rank=0 --nproc_per_node 8 \ --master_adderss $my_address --master_port $my_port main.py (3) mp.spwan 启动 PyTorch引入了torch.multiprocessing.spawn，可以使得单卡、DDP下的外部调用一致，即不用使用torch.distributed.launch。 python xxxx.py一句话搞定DDP模式。 def demo_fn(rank, world_size): dist.init_process_group(&amp;#34;nccl&amp;#34;, rank=rank, world_size=world_size) # lots of code. ... def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) (4) 集群训练步骤 import torch.</description></item><item><title>Transformer研究综述</title><link>https://payne4handsome.github.io/posts/machine-learning/transformer%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/transformer%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/</guid><description>一、基础部分 2017年google发表了一篇All Attention Is All You Need论文, 在机器翻译任务中取得了SOTA 的成绩。论文中提出的Transformer结构取消了传统的Seg2Seg模型中RNN和CNN传统神经网络单元，取而代之代之的Self-Attention（自注意力机制）的计算单元。该计算单元并行化程度高，训练时间短。所以Transformer引起了学术界和工业界的广泛注意。目前已经是NLP领域的标配。随后在2019年google提出基本Transformer的bert模型, 该模型在大量的数据集中通过自监督的训练，然后在特定任务上只需要做少量改动和训练就可以得到非常好的效果，开源的bert模型在11个NLP任务取得SOTA结果。
Transformer除了在NLP任务上表现优异，在CV领域也取得了很多突破。2020年google又发表了一篇 VIT Transformer的论文，实验证明Transformer在imagenet分类任务上取得了SOTA结果。后来CV领域中的各种基本问题，比如目标检测、语义分割、物体追踪、视频等各种任务都用Transformer方法又搞了一遍，基本上也取得了一些不错的结果。
鉴于Transformer在NLP和CV上的巨大成功，本文竟可能详细描述Transformer的基本原理；特定的一些应用，主要是一些经典论文的方法；以及目前Transformer在效率问题上的一些改进的方案。
1.1 Attention 在学习Transformer之前，了解一些基础问题是很有必要。毕竟在没有Transformer之前，学术上在NLP领域也做了大量的研究和成果。我们先从Encoder Decoder和Seq2Seq开始说起。我想大家肯定都听过这两个名称，简单来说就是如下图。 Encoder输入（可以是文字也可以是图像）编程成一个固定长度的向量（content）,那么这个content向量肯定是包含输入的信息的（至于包含多少那就看这个编码了），Decoder根据content解码出我们需要的结果。Encoder Decoder可以是机器翻译问题、语义分割问题等等。那么Seq2Seq（Sequence-to-sequence ）是什么？输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。如下图所示。
从本质上看，Encoder-Decoder和seq2seq好像差不多，但是又有一点区别。
Seq2Seq 属于 Encoder-Decoder 的大范畴 Seq2Seq 更强调目的，Encoder-Decoder 更强调方法 那么这个Encoder-Decoder有什么缺陷呢？
从上面的示意图我们看到，无论输入的信息又多少，Encoder后就剩下一个content向量了，那么这里面有一个缺陷就是这个content向量会丢掉一些信息，特别是输入很大（文本很长图像分辨率很高）的情况下。尽管后面出现的LSTM、GRU等通过门设计的循环神经网络单元，可以一定程度上缓解长距离问题，但是效果有限。
从这里开始，我们要进入文章的正题了，Transformer的核心是Self-Attention，那么在这之前，我们最起码要了解什么是Attention，然后再看是这么在 Attention的基础上加上self的。
1.1.1 NLP中的Attention 由于传统的Encoder-Decoder模型将所有的输入信息编码成一个固定长度的content向量存在长距离问题。那么随之而然的一个做法就是我们在decoder阶段解码$h_t$不仅依赖前一个节点的隐藏状态$h_{t-1}$, 同时依赖Encoder阶段所有的状态，就和我们自已翻译的时候一样。这里有两个经典注意力机制，Bahdanau Attention （2014年提出）和 Luong Attention（2015年）。
1.1.1.1 Bahdanau Attention 注意力机制 示意图如下： 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：
计算对齐向量$a_t$
$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的转态$h_{t-1}$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_{t-1}与每一个Encoder隐状态计算权重后组成的一个向量。
计算$c_t$即content vector
将上一步计算出的$a_t$向量乘以Encoder所有的隐向量。即Encoder所有的隐向量的加权和。
计算Decoder阶段t时刻的输出，$h_t$
将$h^{l-1}{t-1}$与concat（$c_t$， $h{t-1}$）送入多层RNN（最后一层）。其中$h^{l-1}{t-1}$为上一阶段的预测输出。concat（$c_t$， $h{t-1}$）相当于RNN的隐状态。最终将$h_t$过一个softmax就可以预测最终的输出（$y^t$）了。
1.1.1.2 Luong Attention 注意力机制 Luong Attention是在Bahdanau Attention之后提出的。结构更加简洁，效果也更加好一点。 假设现在我们Decoder t时刻。 那么$h_t$隐状态计算过程如下：
计算对齐向量$a_t$
$a_t$的长度与Encoder输出向量的个数相同。$a_t(s)$表示Decoder阶段的状态$h_t$与Encoder阶段第s个隐状态，通过align对齐函数计算出的一个权重。$a_t$就是$h_t$与每一个Encoder隐状态计算权重后组成的一个向量。</description></item><item><title>VAE</title><link>https://payne4handsome.github.io/posts/machine-learning/vae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/vae/</guid><description>Vanilla VAE(Variational Autoencoder) 一、AutoEncoder 回顾 生成模型 最理想的生成就是知道输入样本的分布$P(X)$, 然后我们并不知道该分布。那么可以近似求解。 $P(X) = \Sigma P(X|Z)*P(Z)$。但$P(X|Z), P(Z)$我们同样不知道。但是我们可以用神经网络去学习这两个分布。上图中的latent vector可以看成是$P(Z)$的一个采样，decoder可以看成条件概率$P(X|Z)$。但是我们真的可以采样一个z，然后用加一个decoder来作为我们的生成模型吗？
z是Encoder对应着样本X的输出，如果我们直接用Decoder对z还原，那么最终得到的$\hat{X}$是和X是差不多的，我们需要生成模型是生成一个和X类似的，而不是一模一样的 如果对z做一些扰动，必然加一些噪声，那是不是就可以生成类似但是不一样的东西呢？理论上是可以，但是到目前为止，我们的模型并没有保证这一点（模型还没有学习） 加噪声是一个好的思路，如何加噪声？
让z从一个分布采样（注意不是直接使用encoder的输出），就是噪声。 那不放让z从一个$N(u, \sigma^2)$中采样。那需要知道$u, \sigma^2$, 既然不知道那就用神经网络生成吧。 如果我们按照上述去训练我们的模型，生成方差$\sigma^2$的回倾向于变成0（因为容易学）。那如何加以限制？使z倾向于一个标准正态分布，即$\sigma^2$倾向于1。 如下图 如何监督模型达到该目的，KL loss作为监督信号，KL loss如下 reparameterization trick 思考？ 为什么要正态分布、其它分布可否？ &amp;hellip;.. 待补充完善 参考文献 Auto-Encoding Variational Bayes Tutorial on Variational Autoencoders 变分自编码器 李宏毅深度生成模型</description></item><item><title>机器学习基础之交叉熵和MSE</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8Cmse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8Cmse/</guid><description>机器学习基础之交叉熵与均方误差 我们都知道，对于分类任务，可以选用交叉熵做为模型的损失函数；对于回归任务，可以选用MSE来作为模型的损失函数。那么分类任务能否选用MSE来做为损失函数；回归任务能否选用交叉熵来作为损失函数呢？ 本文只能尽可能尝试回答这个问题，帮助大家有个大概的认识，目前尚无法对其做严格的数学证明。如果大家看到对这个问题有很好的数学证明，欢迎讨论
符号定义：
$N$: 类别数量
$y_i$: 样本onehot编码后label
$p_i$: 模型预测第i个类别的输出
那么可以用交叉熵和MSE来衡量真值和模型预测结果的偏差。公式如下：
交叉熵：$loss_{cross_entropy}=-\sum_i^N y_ilog(p_i)$
MSE: $\frac{1}{N}\sum_i^N (y_i-p_i)^2$
CE是多项式分布的最大似然；
一、为什么分类任务用交叉熵，不能用MSE 1.1 直观感受 假设真实标签为（1,0,0），预测结果一是（0.8,0.1,0.1）, 二是（0.8,0.15,0.05）。那么这两个预测哪个更好一点呢？ 两个预测结果的交叉熵都是$-log0.8=0.223$, 预测一的MSE=0.02, 预测二MSE=0.025。 即MSE任务预测一的结果要好于预测二。MSE潜在的会让预测结果除真实标签以后的标签趋于平均分布。但是实际上我们不能主观的认为预测结果一好于二。
1.2 凹凸性角度 1.2.1 使用sigmod激活、或者softmax，MSE是非凸的 我们知道，如果一个优化问题是凸优化，那么我们是可以找到全局最优解的。但是如果问题是非凸的，那么很有可能找的解是sub-optimal的。 我们用desmos（一个非常好的画图工具）画一个图来说明，对于分类问题，如果用MSE来作为损失函数，它的函数图像是非凸的。 这个例子使用了7个样本，每个样本只具有单个特征。我们可以看到函数图像是非凸的。
在参考文献3中，作者也给出了简单的数学证明，过程如下： 但是以上证明只是证明了最简单的情况（逻辑回归），且只有一个参数$\theta$的情况，如果要证明多元函数是凸的，需要证明黑塞矩阵的正定的，这个很难证明
1.2.2 交叉熵是凸的 还以逻辑回归为例。
$z = wx+b\ a=\sigma(z)\ P(Y=1;w)=a, P(Y=0;w)=1-a$
$\sigma=\frac{1}{1+e^(-x)}$是激活函数
交叉熵为$J(w)=-[y_ilog(a)+(1-y_i)log(1-a)]$
$\frac{\partial J(w)}{\partial w}=-x(y_i-a)$
$\frac{\partial^2 J(w)}{\partial w^2}=-x[-a*(1-a)x]=x^2a*(1-a)$, 其中$a \in (0,1)$, 所以交叉熵的二阶导是大于等于0的。所以交叉熵是凸的。 注意上述证明是特例证明，非严格证明
1.3 参数估计角度 交叉熵多项式分布的极大似然估计
对于样本${(x_1,y_1), (x_2, y_2), &amp;hellip;,(x_N, y_N)}$，使用逻辑回归来分类，那么这批样本的极大似然估计可以用如下式子表达，其中a(x)是sigmod激活
$L(w)=\prod_{i=1}^N(a_w(x_i))^{y_i}(1-a_w(x_i))^{1-y_i}$$
对数似然如下：
$ln(L(w))=\sum_{i=1}^N[y_iln(a(x_i))+(1-y_i)ln(1-a(x_i))]$
上述式子是不是很眼熟，其实就是交叉熵。
其实，对于分类任务不能用MSE的原因是分类需要用sigmod或者softmax来作为激活函数，导致了MSE变成了非凸的函数</description></item></channel></rss>