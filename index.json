[{"content":" Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）一方面表现优秀。主要解决问题有二。\n提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块L：从合成的图像文本对和合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数\n主要分为三个模块\nUnimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后**各自的（指参数不共享）**的finetune captioner模块和filter模块。\ncaptioner: 使用ITC and ITM目标函数\nfilter: 使用LM目标函数\nfinetune使用的是coco数据集\nExperiments CapFilt相关的实验 captioner和filter一起使用效果更好 合成的caption生成有两个方式，两个方式比较 a. Beam: 生成的过程中每次选择概率最大的词 b. Nucleus: 搞一个集合，集合中的词概率加起来大于一个阈值（本论文取0.9），然后从集合随机的选取词 改实验表明：对于合成的caption，多样性是关键\ncaptioner和filter参数是否共享 与SOTA模型比较 图像-文本召回 zero-shot 图像-文本召回 caption 优于clip VQA ","permalink":"https://payne4handsome.github.io/posts/papers/2023-05-22-blip/","summary":"Title: BLIP: 引导语言-图像预训练，用于统一的视觉-语言理解和生成 作者: Junnan Li Dongxu Li Caiming Xiong Steven Hoi；Salesforce Research 发表日期：2022.2 github: https://github.com/salesforce/BLIP 该论文试图解决什么问题？ 目前已经存在的VLP（Vision-Language Pre-training）模型仅仅在理解类任务（understanding-based tasks）或者生成类任务（generation-based tasks）一方面表现优秀。主要解决问题有二。\n提出BLIP，一个新的可以灵活迁移到理解类任务和生成类任务的VLP架构。 (CapFilt): 网络爬取的数据有噪声，该方法可以提升数据的质量。 Key Contributions 提出MED（ultimodal mixture of Encoder-Decoder）架构: 可以有效的多任务预训练和迁移学习。 通过三个视觉-语言目标函数实现：imagetext contrastive learning, image-text matching, and imageconditioned language modeling. 提出CapFilt（Captioning and Filtering）方法: 从有噪声的数据训练。captioner模块：输入网络爬取的图片，输出合成的文本描述（caption 任务）， filter模块L：从合成的图像文本对和合成的图像文本对中删除质量差的数据（noisy captions）. Method 模型结构 note: 颜色相同的模块共享参数\n主要分为三个模块\nUnimodal encoder: 单模态的encoder， 包括图像encoder， 文本encoder Image-grounded text encoder: 通过cross-attention进入视觉信息 Image-grounded text decoder: 用于生成任务 预训练目标函数 Image-Text Contrastive Loss (ITC) 作用：视觉特征空间与文本特征空间对齐（CLIP思想） 实现方式：同一个batch中配对的图像和文本是正样本，不配置的图像和文本是负样本（自已构建正负样本对）。计算cos距离后正样本打高分，负样本打低分。 Image-Text Matching Loss (ITM) 作用：捕获更细粒度的图像文本对齐特征 实现方式：网络最后接一个全连接层做一个二分类任务。note：与ITC不同 Language Modeling Loss (LM) 作用：给定图片生成描述 实现方式：交叉熵 CapFilt 先用网络爬取的数据和人类标注的数据集预训练模型。然后**各自的（指参数不共享）**的finetune captioner模块和filter模块。","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"content":" Title: LoRA: 大语言模型的低秩适配 作者: {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu 发表日期：2021.10 该论文试图解决什么问题？ 提出一个大模型的低秩适配方法去解决全量微调大模型时候需要全量更新模型参数、显存占用很大的问题。\nKey Contributions 对于不同的下游任务，大模型的参数是共享的，变化的只不过是LoRA方法新引入的参数（即B、A参数矩阵）。所以如果有比较多的下游任务，大模型参数只需要保存一份，切换任务的时候也只需要切换一下B、A矩阵即可。大大减少了模型存储的空间和任务切换时候的负载 LoRA方法可以使训练更有效（耗时减少）、减少3倍的显存使用。因为不用保存原始大模型参数的梯度。eg，GPT-3训练需要1.2T显存，使用LoRA方法显存只需要350G左右 不增加推理耗时（上面已经提到） 可以和其他的适配方法结合，比如prefix-tuning Abstract \u0026amp; Introduction \u0026amp; Method NLP模型使用的一个通用范式是先选择一个大的在通用数据集上训练的预训练模型，然后再在一个特定任务上做fine-tune。 但是如果做全量的fine-tune，就要更新模型所有的参数。比如GPT-3有1750亿的参数。fine-tune需要更新1750亿的参数，这个操作是昂贵的。本文提出一个名为LoRA(Low-Rank Adaption)的方法：freeze 预训练模型的参数，在原有的模型结构中插入低秩分解矩阵（rank decomposition matrices）. 该方法可以极大的减少模型的训练参数。\n方法示意图如下 右边橙色的为新引入的可训练的低秩矩阵，其它的为原始模型的参数。数学表达可能更清楚一点。原始模型的前向过程表达为\n$$h = W_0x$$, 修改后的前向过程如下：\n$$h = W_0x+\\Delta Wx=W_ox+BAx$$\nLoRA核心的方法就是改公式。在模型保存的时候可以将$W_0+\\Delta W$保存（即加起来），所以改方法不会增加模型的推理耗时\nExperiments 与不同适配方法在GLUE上的对比 在GPT-3上的适配效果对比 不同方法加大可训练参数量效果对比 Transformer结构为例，LoRA加到哪里更有效？ 参数总量不变（秩r改变），加的地方不一样。实验表明加到$W_q$,$W_v$上效果更好\nr是不是越大越好？ 实验表明，r并不是越大效果越好，对于一些任务，r=4就足够了（取1效果也不错）。对于这个结论论文有一些说明，大致的意思就是r=4的时候，参数量已经够要学习的信息了，再打也是无非是引入冗余的信息罢了。这里解析的可以有失偏颇，感兴趣的参见原文为好。\nCONCLUSION AND FUTURE WORK 关于未来的工作方向。\nLoRA可以和其他迁移方法结合 fine-tuning或者LoRA背后的机制是不清楚的，如何将在预训练的时候学习到的特征迁移到下游任务？作者认为LoRA比full fine-tuning做更好。 作者将LoRA添加到参数矩阵，是通过穷尽、实验的方式，有没有更好的指导原则？ 既然LoRA可以通过添加一个低秩的矩阵就可以取到好的效果，那么原始的参数矩阵是不是也可以降低一下秩？。 第4点确实是一个比较好、且重要的研究方向。\n","permalink":"https://payne4handsome.github.io/posts/papers/2023-05-09-lora-low-rank-adaptation-of-large-lan-guage-models/","summary":"Title: LoRA: 大语言模型的低秩适配 作者: {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu 发表日期：2021.10 该论文试图解决什么问题？ 提出一个大模型的低秩适配方法去解决全量微调大模型时候需要全量更新模型参数、显存占用很大的问题。\nKey Contributions 对于不同的下游任务，大模型的参数是共享的，变化的只不过是LoRA方法新引入的参数（即B、A参数矩阵）。所以如果有比较多的下游任务，大模型参数只需要保存一份，切换任务的时候也只需要切换一下B、A矩阵即可。大大减少了模型存储的空间和任务切换时候的负载 LoRA方法可以使训练更有效（耗时减少）、减少3倍的显存使用。因为不用保存原始大模型参数的梯度。eg，GPT-3训练需要1.2T显存，使用LoRA方法显存只需要350G左右 不增加推理耗时（上面已经提到） 可以和其他的适配方法结合，比如prefix-tuning Abstract \u0026amp; Introduction \u0026amp; Method NLP模型使用的一个通用范式是先选择一个大的在通用数据集上训练的预训练模型，然后再在一个特定任务上做fine-tune。 但是如果做全量的fine-tune，就要更新模型所有的参数。比如GPT-3有1750亿的参数。fine-tune需要更新1750亿的参数，这个操作是昂贵的。本文提出一个名为LoRA(Low-Rank Adaption)的方法：freeze 预训练模型的参数，在原有的模型结构中插入低秩分解矩阵（rank decomposition matrices）. 该方法可以极大的减少模型的训练参数。\n方法示意图如下 右边橙色的为新引入的可训练的低秩矩阵，其它的为原始模型的参数。数学表达可能更清楚一点。原始模型的前向过程表达为\n$$h = W_0x$$, 修改后的前向过程如下：\n$$h = W_0x+\\Delta Wx=W_ox+BAx$$\nLoRA核心的方法就是改公式。在模型保存的时候可以将$W_0+\\Delta W$保存（即加起来），所以改方法不会增加模型的推理耗时\nExperiments 与不同适配方法在GLUE上的对比 在GPT-3上的适配效果对比 不同方法加大可训练参数量效果对比 Transformer结构为例，LoRA加到哪里更有效？ 参数总量不变（秩r改变），加的地方不一样。实验表明加到$W_q$,$W_v$上效果更好\nr是不是越大越好？ 实验表明，r并不是越大效果越好，对于一些任务，r=4就足够了（取1效果也不错）。对于这个结论论文有一些说明，大致的意思就是r=4的时候，参数量已经够要学习的信息了，再打也是无非是引入冗余的信息罢了。这里解析的可以有失偏颇，感兴趣的参见原文为好。\nCONCLUSION AND FUTURE WORK 关于未来的工作方向。\nLoRA可以和其他迁移方法结合 fine-tuning或者LoRA背后的机制是不清楚的，如何将在预训练的时候学习到的特征迁移到下游任务？作者认为LoRA比full fine-tuning做更好。 作者将LoRA添加到参数矩阵，是通过穷尽、实验的方式，有没有更好的指导原则？ 既然LoRA可以通过添加一个低秩的矩阵就可以取到好的效果，那么原始的参数矩阵是不是也可以降低一下秩？。 第4点确实是一个比较好、且重要的研究方向。","title":"LoRA: Low-RanK Adaption Of Large Language Models"},{"content":"本文以实验的形式展示mysql Innodb引擎的四种事务隔离级别的影响。\n四种隔离级别 隔离级别 脏读（Dirty Read） 脏读（Dirty Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 详细说明 以下表（test）解释各个隔离级别，只有两个字段，一个id，一个account\n插入测试数据 关闭mysql自动提交和设置隔离级别\n\u0008查看是否开启自动提交 show variables like \u0026#39;autocommit\u0026#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 未提交读（Read uncommitted） 关闭自动提交、设置对应隔离级别，开启两个会话，下面不在赘述\n会话A 会话B 会话A中插入一条记录，看B中情况 会话A 会话B 结论 我们发现会话A中\u0008事务并没有提交但是在会话B中却可以看到会话A中插入的记录，这种情况就是脏读。 已提交读（Read committed） 设置会话A、B隔离级别为已提交读, 目前会话A和会话B中都只有4条记录，如下：\n会话A 会话B 会话A commit以后，会话B的情况 \u00084. 结论 当会话A中的事务没有提交的时候，会话B中是看不到A中插入的记录，不存在脏读的情况。但是当隔离级别为提交读(Read Committed)时候，会存在不可重复读的情况，实验如下： 会话A和B开启事务，当A中插入一条记录并提交的情况中，会话B的事务中存在前后两次读取不一致的情况。 会话A \u0008会话B在A插入id=5这条记录的前后情况如下： 可重复读（Repeatable read） 会话A、B\u0008设置隔离级别为可重复读（Repeatable read）\n会话A 会话B 结论 我们发现无论是在会话A插入记录并提交之前还是提交之后，会话B中都看不到刚刚A插入的id=7的那条记录，既不存在在隔离级别为Repeatable read中的不可重复读的情况。无论A中插入、更新、删除，B中都是不可见的，即在Repeatable read级别下，B是可重复读的。我们都知道还有一个幻读的问题，为什么都可重复读了，还存在幻读的问题？mysql又是如何解决幻读的问题的呢？ 幻读的定义 官网的定义：\nThe so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a [`SELECT`] (https://dev.mysql.com/doc/refman/5.7/en/select.html \u0026#34;13.2.9 SELECT Syntax\u0026#34;) is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row. 意思就是幻读指在同一个事务中，两次相同的查询结果集不同。那这个又和不可重复读有什么区别呢？确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。\n幻读问题 设置会话A和会话B的隔壁级别为可重复读（Repeatable read）\n当前会话A和会话B的查询情况如下 会话A: 会话B： 下面我们复现一下幻读问题 会话A: 会话B(插入一条记录)： 我们再来看看会话A中情况，我们看看加锁读和不加锁读的区别： 会话A: 我们发现在不加锁时候，是可以重复读的，加锁时候读到了额外的一条记录，这个我们就称之为幻读。那么mysql如何解决幻读的问题呢？答案是gap锁，确切的说是next-key lock。nexy-key lock = record lock + gap lock。比如上面的例子，我们在会话A中执行这条语句的时候（select * from test where account=300;）时候加锁lock，如下：（select * from test where account=300 for update;）。那么会话B在插入（4，300）时候会被阻塞，因为有gap锁。这里因为我们没有在account上加上索引，所以整个表都会被锁（准确的说是accout整个范围都会被锁）。那么mysql何时获取next-key lock？\n何时获取next-key lock 官网描述如下：\n+ For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE, and DELETE statements, locking depends on whether the statement uses a unique index with a unique search condition, or a range-type search condition. + For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it. + For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key locks to block insertions by other sessions into the gaps covered by the range. For information about gap locks and next-key locks, see Section 15.5.1, “InnoDB Locking”. 也就是locking reads，UPDATE和DELETE时，除了对唯一索引的条件外都会获取gap锁或next-key锁。 当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围\n可串行化（Serializable ） 这个级别很简单，读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差。如果你的业务并发的特别少或者没有并发，同时又要求数据及时可靠的话，可以使用这种模式。select在这个级别在Serializable这个级别，还是会加锁的！\nmysql 的隔离级别最难理解的地方在可重复读和幻读的区别，我虽然想尽力去把这里说明白，但是写的时候发现还是很难去描述清楚，这里我也看了很对的blog，也没有发现能把mysql的锁和隔离级别各个方面都讲的很明白的地方，所以要想搞明白这个问题，还是得都看一些资料，集众家之长，下面是我看的比较好的几篇blog\nInnodb中的事务隔离级别和锁的关系-来自美团的技术团队\nmysql REPEATABLE READ对幻读的解决\n官网-幻读\n官网-事务隔离级别\n官网-innodb锁\n我想你把上面的几篇文章都看完了，应该就能理解了\n","permalink":"https://payne4handsome.github.io/posts/basic/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/2022-10-06-mysql-innodb%E4%B8%AD%E7%9A%84%E5%9B%9B%E7%A7%8D%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","summary":"本文以实验的形式展示mysql Innodb引擎的四种事务隔离级别的影响。\n四种隔离级别 隔离级别 脏读（Dirty Read） 脏读（Dirty Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 详细说明 以下表（test）解释各个隔离级别，只有两个字段，一个id，一个account\n插入测试数据 关闭mysql自动提交和设置隔离级别\n\u0008查看是否开启自动提交 show variables like \u0026#39;autocommit\u0026#39;; 打开或关闭自动提交 set autocommit = 1;//打开 set autocommit = 0;//关闭 查看数据库隔离级别 select @@tx_isolation;//当前会话隔离级别 select @@global.tx_isolation;//系统隔离级别 设置数据库隔离级别(当前会话) SET session transaction isolation level read uncommitted; SET session transaction isolation level read committed; SET session transaction isolation level REPEATABLE READ; SET session transaction isolation level Serializable; 未提交读（Read uncommitted） 关闭自动提交、设置对应隔离级别，开启两个会话，下面不在赘述","title":"mysql innodb中的四种事务隔离级别"}]