<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>梯度下降 on Pan'Log</title><link>https://payne4handsome.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><description>Recent content in 梯度下降 on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 28 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://payne4handsome.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/index.xml" rel="self" type="application/rss+xml"/><item><title>梯度下降优化方法概述</title><link>https://payne4handsome.github.io/posts/machine-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/</guid><description>梯度下降是优化神经网络和机器机器学习算法的首选优化方法。本文重度参考SEBASTIAN RUDER的文章。对于英文比较好的同学请直接阅读原文。本文只为个人的学习总结，难免有所欠缺和不足。
一、梯度下降变种 根据训练数据集的大小，梯度下降有三种变体，但是本质是一样的，不一样的是每次使用多少条样本。如果内存一次可以计算所有样本的梯度，称为：批梯度下降（Batch gradient descent）；如果内存一次只允许一个样本，称为：随机梯度下降（Stochastic gradient descent）；大部分时候，内存一次是可以计算部分样本的，称为：最小批梯度下降（Mini-batch gradient descent）。三种变体的数据表达如下：
1.1批梯度下降(Vanilla gradient descent,又称Batch gradient descent) $\theta = \theta - \eta \cdot \nabla_\theta J( \theta)$
1.2随机梯度下降（Stochastic gradient descent） $\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})$
1.3最小批梯度下降（Mini-batch gradient descent） $\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})$
注意，在其他地方并没对上述三种变体做严格区别，统称为SGD（随机梯度下降），下文其余部分，我们也不加区分，统称为SGD
二、梯度下降的几种优化方法 传统的梯度下降法不能保证一个很好的收敛，而且有一些挑战需要被解决。
选择这个合适的学习率是比较困难的。特别是对一个新的模型和新数据集时候，我们是不知道选择什么样的学习率是合适的。只能不断的去尝试。 学习率调度算法可以在训练的过程中去调整模型的学习率。模型一开始的时候可以使用大一点的学习率，后面再使用小一点的学习率去微调模型。更好的方法是一开始也用一个小的学习率去warm-up训练，让参数先适应数据集。但是无论哪种学习率调度算法都需要预先定义调度算法，这种方法也是没有办法很好的适应模型的特征的、 对每一个参数都使用同样的学习率是不合适的。对于稀疏的数据或者特征非常不均衡的数据。最好是使用不同学习率学习不同频率的特征。 另外的挑战是对于高阶非凸的损失函数，往往会陷于局部极值点。还有一种鞍点的情况，模型也是很难学习的。此时损失函数在各个方向的梯度接近于0。SGD是很难逃脱与鞍点或者局部极值点的。 针对上面的一些问题，慢慢出现了一些针对梯度下降的优化方法。 在介绍SGD变种之前。先给出各个变种的一般范式。后天的各个变种优化方法都离不开这个范式。
(1)计算目标函数关于参数的梯度
$g_t = \nabla_\theta J( \theta)$
(2)根据历史梯度计算一阶和二阶动量(二阶指的是梯度的平方) $ m_t = \phi(g_1, g_2, &amp;hellip;, g_t) \ v_t = \psi(g_1, g_2, &amp;hellip;, g_t) $</description></item></channel></rss>