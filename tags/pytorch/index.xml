<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>pytorch on Pan'Log</title><link>https://payne4handsome.github.io/tags/pytorch/</link><description>Recent content in pytorch on Pan'Log</description><image><title>Pan'Log</title><url>https://payne4handsome.github.io/papermod-cover.png</url><link>https://payne4handsome.github.io/papermod-cover.png</link></image><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 15 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://payne4handsome.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml"/><item><title>pytorch ddp</title><link>https://payne4handsome.github.io/posts/machine-learning/pytorch-ddp/</link><pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate><guid>https://payne4handsome.github.io/posts/machine-learning/pytorch-ddp/</guid><description>(1) 关键概念
world_size: 集群中所有GPU的数量 rank: 范围[0, world_size-1], 表示GPU的编号 local_rank: GPU在每台机器上的编号 比如，两台机器，每台机器4块卡，那么world_size= 2*4, rank 取值范围 [0,1,2,3,4,5,6,7]， local_rank 取值范围[0,1,2,3] (2) torch.distributed.launch 启动集群参数
&amp;ndash;nnodes: 一共多少台机器 &amp;ndash;node_rank: 当前机器编号 &amp;ndash;nproc_per_node: 每台机器多少个进程 &amp;ndash;master_adderss: master节点ip地址 &amp;ndash;master_port: master节点端口 master节点的node_rank必须为0 command example:
python -m torch.distributed.launch --nnodes=2 --node_rank=0 --nproc_per_node 8 \ --master_adderss $my_address --master_port $my_port main.py (3) mp.spwan 启动 PyTorch引入了torch.multiprocessing.spawn，可以使得单卡、DDP下的外部调用一致，即不用使用torch.distributed.launch。 python xxxx.py一句话搞定DDP模式。 def demo_fn(rank, world_size): dist.init_process_group(&amp;#34;nccl&amp;#34;, rank=rank, world_size=world_size) # lots of code. ... def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) (4) 集群训练步骤 import torch.</description></item></channel></rss>